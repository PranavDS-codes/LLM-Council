import asyncio
import json
import os
import random
from typing import Optional, Any
from openai import AsyncOpenAI
from openai import AsyncOpenAI
from config import USE_MOCK_MODE, OPENROUTER_API_KEY, OPENROUTER_BASE_URL, MODEL_MAP, OPENROUTER_SITE_URL, OPENROUTER_APP_NAME

class LLMClient:
    def __init__(self):
        self.mock_mode = USE_MOCK_MODE
        self.openai_client = AsyncOpenAI(
            api_key=OPENROUTER_API_KEY,
            base_url=OPENROUTER_BASE_URL
        ) if not self.mock_mode else None
        
    async def generate(self, prompt: str, schema: Optional[Any] = None, model: Optional[str] = None) -> str:
        """
        Generates a response from the LLM. 
        If 'schema' is provided, it attempts to enforce JSON output.
        """
        if self.mock_mode:
            return await self._mock_generate(prompt, schema)
        
        return await self._real_generate(prompt, schema, model)

    # ... (mock methods generally unchanged or can be kept as is if not replacing whole file) ...

    async def _mock_generate(self, prompt: str, schema: Optional[Any] = None) -> str:
        await asyncio.sleep(0.5) # Simulate latency
        
        if "You are an impartial Senior Quality Assurance Judge" in prompt:
             return json.dumps({
                "winner_id": "Response A",
                "rankings": ["Response A", "Response B", "Response C"],
                "reasoning": "Response A provided the most depth.",
                "flaws": {"Response B": "Too brief", "Response C": "Hallucinated data"},
                "scores": {"Response A": 9, "Response B": 7, "Response C": 5}
            })
        elif "You are the Chief Solutions Architect" in prompt:
            return json.dumps({
                "structure": ["Introduction", "Analysis", "Conclusion"],
                "tone_guidelines": "Professional and objective",
                "missing_facts_to_add": ["Specific dates of events"],
                "critique_integration": "Incorporate feedback on brevity."
            })
        elif "You are the Finalizer" in prompt:
            return "This is the final comprehensive answer generated by the Council."
        else:
            return f"Mock Response to: {prompt[:50]}..."

    async def _real_generate(self, prompt: str, schema: Optional[Any] = None, model: Optional[str] = None) -> str:
        # Use provided model or default to generator
        target_model = model if model else MODEL_MAP.get("generator_1", "nvidia/nemotron-nano-12b-v2-vl:free")
        
        # Prepare kwargs
        kwargs = {
            "model": target_model,
            "messages": [{"role": "user", "content": prompt}],
            "extra_headers": {
                "HTTP-Referer": OPENROUTER_SITE_URL,
                "X-Title": OPENROUTER_APP_NAME,
            }
        }
        
        # Enable JSON mode if schema is requested
        if schema:
            kwargs["response_format"] = {"type": "json_object"}
        
        # Retry logic parameters
        max_retries = 3
        base_delay = 2
        
        for attempt in range(max_retries + 1):
            try:
                response = await self.openai_client.chat.completions.create(**kwargs)
                return response.choices[0].message.content
            except Exception as e:
                error_msg = str(e)
                # Check if it is the last attempt
                if attempt == max_retries:
                    return f"Error calling OpenRouter after {max_retries} retries: {error_msg}"
                
                # Check if error is related to rate limits or overloaded servers
                if "429" in error_msg or "500" in error_msg or "503" in error_msg:
                    delay = base_delay * (2 ** attempt) + (random.random() * 0.5)
                    print(f"[Warning] Rate limit/Server error encountered. Retrying in {delay:.2f}s... (Attempt {attempt + 1}/{max_retries})")
                    await asyncio.sleep(delay)
                else:
                    # If it's another error (e.g. auth), fail fast
                    return f"Error calling OpenRouter: {error_msg}"
